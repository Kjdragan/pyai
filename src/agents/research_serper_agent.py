"""
Serper-based Research Agent for comprehensive search capabilities.
Uses Google Search via Serper API for high-quality research results.
"""

import asyncio
from typing import List, Optional
from datetime import datetime
from pydantic_ai import Agent, RunContext
from pydantic_ai.models.openai import OpenAIModel

from models import ResearchPipelineModel, ResearchItem, AgentResponse
from config import config
from agents.query_expansion import expand_query_to_subquestions as expand_query_to_subquestions_llm
from agents.content_cleaning_agent import clean_research_item_content
from agents.quality_grader import quality_grader
from utils.time_provider import today_str, now


class SerperResearchDeps:
    """Dependencies for Serper Research Agent."""
    
    def __init__(self):
        self.api_key = config.SERPER_API_KEY
        self.max_results = config.MAX_RESEARCH_RESULTS
        self.current_date = today_str()
        self.research_results: List[ResearchItem] = []
        self.sub_questions: List[str] = []


async def expand_query_to_subquestions(query: str) -> List[str]:
    """Delegate to centralized LLM-based query expansion to ensure high-quality, search-optimized sub-questions."""
    return await expand_query_to_subquestions_llm(query)


# PERFORMANCE OPTIMIZATION: Enhanced scraping with pre-flight checks moved to utils
# This eliminates duplicate code and provides intelligent gating to avoid blocked sites
from utils.intelligent_scraper import scrape_url_content_detailed

async def scrape_url_content(url: str, max_chars: int = 2000) -> str:
    """Enhanced scraping with pre-flight checks and intelligent error handling."""
    result = await scrape_url_content_detailed(url, max_chars)
    
    # Log detailed information for debugging
    if not result.success:
        if result.was_blocked:
            print(f"üö´ BLOCKED: {url} - {result.block_reason} (saved {result.processing_time:.2f}s)")
        else:
            print(f"‚ùå FAILED: {url} - {result.error_reason} (after {result.processing_time:.2f}s)")
    else:
        print(f"‚úÖ SUCCESS: {url} - {len(result.content)} chars (in {result.processing_time:.2f}s)")
    
    return result.content


async def search_serper(query: str, api_key: str, max_results: int = 10) -> List[ResearchItem]:
    """Perform search using Serper API with URL content scraping."""
    
    if not api_key:
        return []
    
    url = "https://google.serper.dev/search"
    
    headers = {
        "X-API-KEY": api_key,
        "Content-Type": "application/json"
    }
    
    payload = {
        "q": query,
        "num": max_results,
        "hl": "en",
        "gl": "us"
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # Process organic results with intelligent quality grading
            raw_results = []
            for i, item in enumerate(data.get("organic", [])[:max_results]):
                basic_snippet = item.get("snippet", "")
                source_url = item.get("link", "")
                
                # Create initial research item for quality evaluation (no scraping yet)
                result = ResearchItem(
                    query_variant=query,
                    source_url=source_url,
                    title=item.get("title", ""),
                    snippet=basic_snippet,
                    relevance_score=None,  # Will be calculated by quality grader
                    timestamp=now(),
                    content_scraped=False,
                    scraping_error=None,
                    content_length=0,
                    scraped_content=None
                )
                raw_results.append(result)
            
            # Apply intelligent quality grading to determine what to scrape
            print(f"üìä SERPER DEBUG: Applying quality grading to {len(raw_results)} results")
            graded_results = quality_grader.grade_result_batch(
                raw_results, query, "serper", max_scrape_count=config.MAX_SCRAPING_PER_QUERY
            )
            
            # Now perform selective scraping based on quality grades
            final_results = []
            for result in graded_results:
                should_scrape = result.metadata.get('should_scrape', False)
                
                if should_scrape and result.source_url:
                    print(f"üöÄ SERPER DEBUG: High-quality result - scraping {result.source_url} (score: {result.relevance_score:.2f})")
                    scraped_content = await scrape_url_content(result.source_url, max_chars=10000)
                    
                    if scraped_content:
                        result.scraped_content = scraped_content
                        result.content_scraped = True
                        result.content_length = len(scraped_content)
                        # Preserve exact raw scraped text for quote retention analysis
                        result.raw_content = scraped_content
                        result.raw_content_length = len(scraped_content)
                        print(f"‚úÖ SERPER DEBUG: Successfully scraped {result.content_length} chars from {result.source_url}")
                    else:
                        result.scraping_error = "Failed to scrape content"
                        print(f"‚ùå SERPER DEBUG: Failed to scrape content from {result.source_url}")
                else:
                    skip_reason = result.metadata.get('skip_reason', 'Quality threshold not met')
                    print(f"‚è≠Ô∏è  SERPER DEBUG: Skipping scraping for {result.source_url} - {skip_reason}")
                
                final_results.append(result)
            
            # Log quality summary
            quality_summary = quality_grader.get_quality_summary(final_results)
            print(f"üìà SERPER QUALITY SUMMARY: {quality_summary}")
            
            return final_results
            
    except Exception as e:
        print(f"Serper search error for '{query}': {str(e)}")
        return []


def clean_and_format_results(results: List[ResearchItem], original_query: str) -> List[ResearchItem]:
    """Clean and format search results while preserving scraping metadata."""
    
    # Remove duplicates by URL
    seen_urls = set()
    cleaned = []
    
    for result in results:
        if result.source_url not in seen_urls and result.source_url:
            seen_urls.add(result.source_url)
            
            # Basic cleaning - remove excessive whitespace from snippet only (keep it short)
            cleaned_snippet = " ".join(result.snippet.split())
            if len(cleaned_snippet) > 500:
                cleaned_snippet = cleaned_snippet[:497] + "..."
            
            # Clean scraped content but DON'T truncate it - keep it full for report generation
            cleaned_scraped_content = None
            if result.scraped_content:
                # Just clean whitespace, but preserve full length
                cleaned_scraped_content = " ".join(result.scraped_content.split())
            
            # Create cleaned item - PRESERVE ALL SCRAPING METADATA AND FULL CONTENT
            cleaned_item = ResearchItem(
                query_variant=result.query_variant,
                source_url=result.source_url,
                title=result.title.strip(),
                snippet=cleaned_snippet,  # Short snippet for display
                relevance_score=result.relevance_score,
                timestamp=result.timestamp,
                # CRITICAL: Preserve scraping metadata
                content_scraped=result.content_scraped,
                scraping_error=result.scraping_error,
                content_length=result.content_length,
                scraped_content=cleaned_scraped_content,  # Full content for report generation (NOT truncated)
                # Preserve raw content and cleaning metadata
                raw_content=result.raw_content,
                raw_content_length=result.raw_content_length,
                content_cleaned=result.content_cleaned,
                original_content_length=result.original_content_length,
                cleaned_content_length=result.cleaned_content_length,
                metadata=result.metadata
            )
            cleaned.append(cleaned_item)
    
    # Sort by relevance score (descending)
    cleaned.sort(key=lambda x: x.relevance_score or 0, reverse=True)
    
    return cleaned[:config.MAX_RESEARCH_RESULTS]


# Create Serper Research Agent with instrumentation
serper_research_agent = Agent(
    model=OpenAIModel(config.RESEARCH_MODEL),
    deps_type=SerperResearchDeps,
    output_type=ResearchPipelineModel,
    instrument=True,  # Enable Pydantic AI tracing
    system_prompt=f"""
    You are a research coordination agent that processes Google search results via Serper API.
    
    Today's date is: {today_str()}
    
    When asked to research something:
    1. Call your perform_serper_research tool with the exact user query
    2. The tool returns raw data: {{"original_query": str, "sub_queries": list, "raw_results": list, "processing_time": float}}
    3. Convert this raw data into a ResearchPipelineModel with these exact fields:
       - original_query: use the original_query from tool
       - sub_queries: use the sub_queries from tool  
       - results: convert each item in raw_results to ResearchItem format
       - pipeline_type: set to "serper"
       - total_results: count of items in raw_results
       - processing_time: use processing_time from tool
    
    CRITICAL: When converting raw_results to ResearchItem format, preserve ALL data exactly as-is:
    - scraped_content field must contain the complete raw scraped text without any summarization, processing, or modification
    - Do NOT summarize, clean, or modify the scraped_content in any way
    - Simply pass through all fields from raw_results unchanged
    - The scraped_content should be the full original scraped text, not a summary
    
    When expanding queries, follow this universal approach:
    
    STEP 1: First analyze the query to understand its nature and context:
    - Is it about a product/service, historical event, current news, scientific concept, business topic, etc.?
    - What domain does it belong to (technology, health, politics, economics, science, etc.)?
    - What might be the user's intent (learning, comparison, current status, trends, etc.)?
    
    STEP 2: Based on the query analysis, generate 3 diverse sub-questions that explore different aspects:
    - For PRODUCTS/SERVICES: features/specs, market position/reviews, pricing/availability
    - For HISTORICAL TOPICS: background/causes, key events/timeline, impact/consequences
    - For CURRENT NEWS: recent developments, context/background, implications/reactions
    - For SCIENTIFIC CONCEPTS: fundamental principles, applications/examples, latest research
    - For BUSINESS TOPICS: current status, market dynamics, future outlook
    - For GENERAL TOPICS: adapt based on what would provide comprehensive coverage
    
    Examples across different query types:
    
    Query: "iPhone 15 Pro" (Product)
    Sub-questions:
    1. What are the key features, specifications, and technical improvements of the iPhone 15 Pro?
    2. How does the iPhone 15 Pro compare to competitors and previous iPhone models in reviews and market reception?
    3. What is the pricing, availability, and market performance of the iPhone 15 Pro?
    
    Query: "Fall of Berlin Wall" (Historical)
    Sub-questions:
    1. What were the political and economic factors that led to the fall of the Berlin Wall in 1989?
    2. What were the key events and timeline surrounding the fall of the Berlin Wall?
    3. What were the immediate and long-term consequences of the Berlin Wall's fall on Germany and Europe?
    
    CRITICAL: Return only the structured ResearchPipelineModel. Do not add commentary, explanations, or extra text.
    Just the properly formatted structured data based on what your tool returned.
    """,
    retries=config.MAX_RETRIES
)


@serper_research_agent.tool
async def expand_query_intelligently(ctx: RunContext[SerperResearchDeps], query: str) -> List[str]:
    """Tool to intelligently expand a query into 3 diverse sub-questions using LLM analysis.
    
    First analyzes the query type and context, then generates appropriate sub-questions
    that explore different aspects based on the query's nature (product, historical, news, etc.).
    """
    # This tool leverages the agent's LLM capabilities through the system prompt
    # The actual expansion logic is handled by the LLM based on the guidelines in the system prompt
    return await expand_query_to_subquestions(query)


@serper_research_agent.tool
async def perform_serper_research(ctx: RunContext[SerperResearchDeps], query: str) -> ResearchPipelineModel:
    """Tool to perform comprehensive Serper research and return ResearchPipelineModel."""
    try:
        print(f"üîç SERPER TOOL DEBUG: Starting research for query: {query}")
        
        if not ctx.deps.api_key:
            print(f"‚ùå SERPER TOOL DEBUG: API key not configured")
            return ResearchPipelineModel(
                original_query=query,
                sub_queries=[],
                results=[],
                pipeline_type="serper",
                total_results=0,
                processing_time=0.0
            )
        
        print(f"‚úÖ SERPER TOOL DEBUG: API key configured, current date: {ctx.deps.current_date}")
        
        # Check if pre-generated sub-queries are provided in the query (centralized approach)
        # This prevents duplicate query expansion across multiple research APIs
        import re
        sub_query_match = re.search(r'Use these pre-generated sub-queries.*?:\n((?:\d+\.\s+.*\n?)+)', query, re.DOTALL)
        
        if sub_query_match:
            # Extract pre-generated sub-queries from orchestrator
            sub_queries_text = sub_query_match.group(1)
            sub_questions = []
            for line in sub_queries_text.strip().split('\n'):
                if line.strip() and re.match(r'\d+\.\s+', line):
                    sub_question = re.sub(r'^\d+\.\s+', '', line.strip())
                    sub_questions.append(sub_question)
            print(f"üéØ SERPER TOOL DEBUG: Using {len(sub_questions)} pre-generated sub-queries from orchestrator")
            print(f"üìù Sub-queries: {sub_questions}")
        else:
            # CRITICAL FIX: Use single query as fallback instead of generating new sub-queries
            # This prevents the infinite loop that was causing 58+ API calls
            sub_questions = [query]
            print(f"üéØ SERPER TOOL DEBUG: Using single query fallback (no pre-generated sub-queries found)")
        
        # Perform parallel searches with full result capacity per sub-query
        # This allows comprehensive research coverage instead of artificially limiting results
        # Execute searches with optional bounded parallelism
        if config.RESEARCH_PARALLELISM_ENABLED:
            sem = asyncio.Semaphore(config.SERPER_MAX_CONCURRENCY)

            async def _bounded_search(q: str):
                async with sem:
                    return await search_serper(q, ctx.deps.api_key, ctx.deps.max_results)

            search_tasks = [_bounded_search(question) for question in sub_questions]
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        else:
            # Previous behavior: unbounded gather (usually small N)
            search_tasks = [
                search_serper(question, ctx.deps.api_key, ctx.deps.max_results)
                for question in sub_questions
            ]
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)
        print(f"üîç SERPER TOOL DEBUG: Received {len(search_results)} search results")
        
        # Combine results
        all_results = []
        for i, results in enumerate(search_results):
            if isinstance(results, list):
                print(f"üìä SERPER TOOL DEBUG: Search {i+1} returned {len(results)} results")
                all_results.extend(results)
            else:
                print(f"‚ö†Ô∏è SERPER TOOL DEBUG: Search {i+1} returned non-list: {type(results)}")
        
        print(f"üìä SERPER TOOL DEBUG: Total combined results: {len(all_results)}")
        
        # PERFORMANCE OPTIMIZATION: Apply programmatic garbage filtering before expensive LLM cleaning
        # This prevents wasting compute resources on low-quality content
        scraped_items = [item for item in all_results if item.scraped_content and item.content_scraped]
        if scraped_items:
            print(f"üóëÔ∏è  Applying programmatic garbage filtering to {len(scraped_items)} scraped items")
            
            # Import and apply content quality filter
            from utils.content_quality_filter import content_filter
            
            filtered_items = []
            garbage_count = 0
            
            for item in scraped_items:
                # Store pre-filtering content state for visibility
                item.pre_filter_content = item.scraped_content
                item.pre_filter_content_length = len(item.scraped_content or "")
                
                # Apply quality filtering using comprehensive programmatic analysis
                should_filter, filter_reason = content_filter.should_filter_content(
                    content=item.scraped_content,
                    url=item.source_url or "",
                    title=item.title or "",
                    quality_threshold=config.GARBAGE_FILTER_THRESHOLD  # Configurable threshold
                )
                
                # Get detailed quality analysis for insights
                quality_analysis = content_filter.analyze_content_quality(
                    content=item.scraped_content,
                    url=item.source_url or "",
                    title=item.title or ""
                )
                item.quality_score = quality_analysis['overall_quality_score']
                
                if should_filter:
                    garbage_count += 1
                    print(f"üöÆ GARBAGE FILTERED: {item.source_url} - {filter_reason} (Quality: {item.quality_score:.2f})")
                    
                    # Mark as filtered with detailed information
                    item.garbage_filtered = True
                    item.filter_reason = filter_reason
                    item.post_filter_content = None  # No content passed filtering
                    item.post_filter_content_length = 0
                    item.content_cleaned = False  # Skip expensive LLM cleaning
                    
                    # Store filtering metadata for transparency
                    if not hasattr(item, 'metadata') or not item.metadata:
                        item.metadata = {}
                    item.metadata['quality_analysis'] = quality_analysis
                    item.metadata['filtered_as_garbage'] = True
                else:
                    # Content passed filtering
                    item.garbage_filtered = False
                    item.filter_reason = None
                    item.post_filter_content = item.scraped_content
                    item.post_filter_content_length = len(item.scraped_content or "")
                    filtered_items.append(item)
                
                # PERFORMANCE OPTIMIZATION: Truncate pre-filter content after processing for log efficiency
                # Keep full content for processing, but truncate stored version to save space
                if item.pre_filter_content and len(item.pre_filter_content) > 500:
                    item.pre_filter_content = item.pre_filter_content[:500] + "...[TRUNCATED FOR LOG EFFICIENCY]"
                    
            # Generate detailed filtering summary for insights
            total_pre_filter_chars = sum(item.pre_filter_content_length or 0 for item in all_results if hasattr(item, 'pre_filter_content_length') and item.pre_filter_content_length)
            total_post_filter_chars = sum(item.post_filter_content_length or 0 for item in filtered_items)
            chars_filtered = total_pre_filter_chars - total_post_filter_chars
            filter_efficiency = (chars_filtered / total_pre_filter_chars * 100) if total_pre_filter_chars > 0 else 0
            
            print(f"‚úÖ GARBAGE FILTERING SUMMARY:")
            print(f"   ‚Ä¢ Filtered {garbage_count}/{len(scraped_items)} items ({garbage_count/len(scraped_items)*100:.1f}%)")
            print(f"   ‚Ä¢ Removed {chars_filtered:,} characters ({filter_efficiency:.1f}% reduction)")
            print(f"   ‚Ä¢ {len(filtered_items)} quality items proceeding to LLM cleaning")
            print(f"   ‚Ä¢ Estimated API cost savings: ${garbage_count * 0.02:.2f} (prevented garbage processing)")
            
            scraped_items = filtered_items  # Use only non-garbage items for LLM cleaning
            
        # PERFORMANCE OPTIMIZATION: Clean scraped content using batched nano model processing
        # This removes boilerplate text before report generation with true parallel batch processing
        if scraped_items:
            print(f"üßπ Starting BATCHED content cleaning for {len(scraped_items)} Serper results")
            cleaning_start = asyncio.get_event_loop().time()
            
            # Use batched content cleaning for true parallelization
            from agents.content_cleaning_agent import clean_multiple_contents_batched
            content_tuples = [(item.scraped_content, query, item.source_url or "unknown") for item in scraped_items]
            
            # Execute batched cleaning with real parallel processing
            cleaned_results = await clean_multiple_contents_batched(content_tuples, batch_size=4)
            
            # Apply results back to research items with proper metrics
            for item, (cleaned_content, success) in zip(scraped_items, cleaned_results):
                if success:
                    # Preserve raw content before overwriting
                    if not getattr(item, 'raw_content', None):
                        item.raw_content = item.scraped_content
                        item.raw_content_length = len(item.scraped_content)
                    
                    # Update with cleaned content and correct metrics
                    original_length = len(item.scraped_content)
                    item.scraped_content = cleaned_content
                    item.content_cleaned = True
                    item.original_content_length = original_length
                    item.cleaned_content_length = len(cleaned_content)
                    item.content_length = len(cleaned_content)  # Fix: should reflect cleaned length
                    
                    # Add quote metrics
                    def _count_quotes(text):
                        quote_chars = ['"', "'", '"', '"', ''', ''']
                        return sum(text.count(ch) for ch in quote_chars)
                    
                    if not hasattr(item, 'metadata') or not item.metadata:
                        item.metadata = {}
                    item.metadata.setdefault('quote_metrics', {})
                    item.metadata['quote_metrics'].update({
                        'quote_chars_before': _count_quotes(item.raw_content or ""),
                        'quote_chars_after': _count_quotes(cleaned_content),
                        'quote_chars_delta': _count_quotes(cleaned_content) - _count_quotes(item.raw_content or "")
                    })
                else:
                    item.content_cleaned = False
            
            cleaning_time = asyncio.get_event_loop().time() - cleaning_start
            success_count = sum(1 for _, success in cleaned_results if success)
            print(f"‚úÖ BATCHED content cleaning completed: {success_count}/{len(scraped_items)} successful in {cleaning_time:.2f}s")
        else:
            print(f"‚è≠Ô∏è No scraped content to clean from Serper results")
        
        # Clean and format results
        cleaned_results = clean_and_format_results(all_results, query)
        print(f"‚ú® SERPER TOOL DEBUG: After cleaning: {len(cleaned_results)} results")
        
        # Store results in context for agent access
        ctx.deps.research_results = cleaned_results
        ctx.deps.sub_questions = sub_questions
        
        # CRITICAL FIX: Return ResearchPipelineModel instead of dict to prevent infinite agent loop
        return ResearchPipelineModel(
            original_query=query,
            sub_queries=sub_questions,
            results=cleaned_results,  # Already ResearchItem objects
            pipeline_type="serper",
            total_results=len(cleaned_results),
            processing_time=0.0  # Will be calculated at agent level
        )
               
    except Exception as e:
        print(f"‚ùå SERPER TOOL DEBUG: Exception caught: {str(e)}")
        import traceback
        print(f"‚ùå SERPER TOOL DEBUG: Traceback: {traceback.format_exc()}")
        return ResearchPipelineModel(
            original_query=query,
            sub_queries=[],
            results=[],
            pipeline_type="serper",
            total_results=0,
            processing_time=0.0
        )


# DELETED: Legacy function removed - use serper_research_agent Pydantic AI agent instead